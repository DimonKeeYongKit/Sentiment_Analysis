{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These is the post:\n",
      "  1)We're on again for #buildinpublic #2\n",
      " of the @supabase_io python library on 30th May 9pm-10pm GMT +8 /6am PST.\n",
      "\n",
      "Thi… https://t.co/qtNGqRc2kr\n",
      "\n",
      "\n",
      "\n",
      "  2)RT @cepribwriters: Get quality grades for your online classes and delivery on time\n",
      "#maths \n",
      "#Paperpay\n",
      "#python\n",
      "#chemistry \n",
      "#WomenWhoCode \n",
      "#As…\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "*** tk version 8.6.9 detected.... patching ttk treeview code ***\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import re, string, random\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "pd.set_option(\"max_colwidth\", 250)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "clean_tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "# remove all the useless item\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        #remove tag, hashtags, hyperlink\n",
    "        cleanText(token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n' #NOUN\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v' #VERB\n",
    "        else:\n",
    "            pos = 'a' #ADJ\n",
    "            \n",
    "        # this function is to converting a word to its canonical form such as running, ran --> run\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        # remove stop word\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9_-]+', '', text) # remove tag\n",
    "    text = re.sub(r'#[A-Za-z0-9_-]+', '', text) #remove hashtag\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) #remove hyperlink\n",
    "    text = re.sub(r'[\\n]+', '', text) #remove nextline\n",
    "    text = re.sub(r'RT : ', '', text) #remove RT\n",
    "    return text\n",
    "\n",
    "#takes a list of tweets as an argument to provide a list of words in all of the tweet tokens joined.\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token #yield is use in a generator, func are almost same with return\n",
    "\n",
    "            \n",
    "# preparation for passing data list into model\n",
    "#convert into dictionary(word as key, true as value)\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "        \n",
    "        \n",
    "def preRUN():\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    # text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    # tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    # print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "    global classifier\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    global A\n",
    "    A = classify.accuracy(classifier, test_data)\n",
    "    \n",
    "def NBSentimentAnalysis(custom_tweet):\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "    answer = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "    return answer\n",
    "    \n",
    "def searchInTwitter():\n",
    "    # get the API key\n",
    "    log = pd.read_csv('TwitterAPI.csv')\n",
    "\n",
    "    # Authentication\n",
    "    consumerKey = log['key'][0]\n",
    "    consumerSecret = log['secret'][0]\n",
    "    accessToken = log['token'][0]\n",
    "    accessTokenSecret = log['tokenSecret'][0]\n",
    "\n",
    "    # Create the authentication object\n",
    "    auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "    # Set the access token and token secret\n",
    "    auth.set_access_token(accessToken, accessTokenSecret)\n",
    "    #create api object\n",
    "    api = tweepy.API(auth, wait_on_rate_limit = True)\n",
    "\n",
    "    \n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    # Define the window's contents\n",
    "    layout = [  [sg.Text(\"Please enter keyword or hashtag to search: \")],\n",
    "              [sg.Input()],\n",
    "              [sg.Text(\"Please enter how many tweets to analyze: \")],\n",
    "              [sg.Input()],\n",
    "              [sg.Button('OK')] ]\n",
    "\n",
    "    # Create the window\n",
    "    window = sg.Window('Sentiment Analysis System', layout)\n",
    "    # Display and interact with the Window\n",
    "    event, values = window.read()\n",
    "    # Finish up by removing from the screen\n",
    "    window.close()\n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    \n",
    "    # extract what item what quantity to crawl\n",
    "    global keyword\n",
    "    keyword = values[0]\n",
    "    noOfTweet = int(values[1])\n",
    "    posts = tweepy.Cursor(api.search, q=keyword, lang=\"en\").items(noOfTweet)\n",
    "\n",
    "    print(\"These is the post:\")\n",
    "    i = 1\n",
    "    for post in posts:\n",
    "        tweet_list.append(post.text)\n",
    "        print(\"  \"+str(i)+\")\"+post.text)\n",
    "        print(\"\\n\\n\");\n",
    "        i += 1\n",
    "\n",
    "def getSubjectivity(text):\n",
    "    #the range of [0,1], 1 means a public opinion and not a factual information\n",
    "    return TextBlob(text).sentiment.subjectivity \n",
    "\n",
    "def getPolarity(text):\n",
    "    #the range of [-1,1], -1 means negatif , 1 means positif\n",
    "    return TextBlob(text).sentiment.polarity \n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    elif score > 0:\n",
    "        return 'Positive'\n",
    "    \n",
    "def uiNB():\n",
    "    window2 = sg.Window(\"Percentage of TB_Analysis\", [  [sg.Text(\"Positive percentage : \"+ str(posp))],\n",
    "          [sg.Text(\"Neutral percentage : \"+ str(neup))],\n",
    "          [sg.Text(\"Negative percentage : \"+ str(negp))],\n",
    "          [sg.Button('Show BarChart')],\n",
    "          [sg.Button('Show PieChart')] ])\n",
    "    while True:\n",
    "        e,v = window2.read()\n",
    "        if e == \"Show BarChart\":\n",
    "            getBarChartTB()\n",
    "        elif e == \"Show PieChart\":\n",
    "            getPieChart()\n",
    "        elif e == sg.WIN_CLOSED:\n",
    "            break\n",
    "\n",
    "def uiTB():\n",
    "    window3 = sg.Window(\"Percentage of NB_Analysis\", [  [sg.Text(\"Positive percentage : \"+ str(posp))],\n",
    "          [sg.Text(\"Neutral percentage : \"+ str(neup))],\n",
    "          [sg.Text(\"Negative percentage : \"+ str(negp))],\n",
    "          [sg.Button('Show BarChart')],\n",
    "          [sg.Button('Show PieChart')] ])\n",
    "    while True:\n",
    "        e,v = window3.read()\n",
    "        if e == \"Show BarChart\":\n",
    "            getBarChartNB()\n",
    "        elif e == \"Show PieChart\":\n",
    "            getPieChart()\n",
    "        elif e == sg.WIN_CLOSED:\n",
    "            break\n",
    "    \n",
    "def TextBlobSentimentAnalysis():\n",
    "    global df\n",
    "    df = pd.DataFrame([tweet for tweet in tweet_list], columns=['Tweet'])\n",
    "    df['Tweet'] = df['Tweet'].apply(cleanText)\n",
    "    df['Subjectivity'] = df['Tweet'].apply(getSubjectivity)\n",
    "    df['Polarity'] = df['Tweet'].apply(getPolarity)\n",
    "    df['TB_Analysis'] = df['Polarity'].apply(getAnalysis)\n",
    "    df['NB_Analysis'] = df['Tweet'].apply(NBSentimentAnalysis)\n",
    "    print('-----------------------------------------------------')\n",
    "    df.head()\n",
    "    global ddf\n",
    "    ddf = pd.DataFrame([tweet for tweet in tweet_list], columns=['Tweet'])\n",
    "    ddf['Tweet'] = ddf['Tweet'].apply(cleanText)\n",
    "  \n",
    "    \n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    sg.set_options(auto_size_buttons=True)\n",
    "    data = []\n",
    "    layout = [\n",
    "        [sg.Text(\"List of tweet\")],\n",
    "        [sg.Table(values=ddf.values.tolist(),\n",
    "                  headings=list(ddf.columns.values),\n",
    "                  display_row_numbers=True, col_widths=500,\n",
    "                  num_rows=max(10,len(data))\n",
    "                 )],\n",
    "        [sg.Button(\"Show system accuracy\"),sg.Button(\"Show the sentiment analysis\"), sg.Button(\"Cancel\")]\n",
    "    ]\n",
    "    window = sg.Window('Tweet Table', layout, grab_anywhere=False)\n",
    "    global posp\n",
    "    global neup\n",
    "    global negp\n",
    "\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == \"Show system accuracy\":\n",
    "            print(\"The accuracy is :\",str(A))\n",
    "            classifier.show_most_informative_features(20)\n",
    "        elif event == \"Show the sentiment analysis\":\n",
    "            \n",
    "            window4 = sg.Window('Result of Analysis',\n",
    "                                [ [sg.Text(\"List of tweet with analysis\")],\n",
    "                                [sg.Table(values=df.values.tolist(),\n",
    "                                          headings=list(df.columns.values),\n",
    "                                          display_row_numbers=True,\n",
    "                                          auto_size_columns=True,\n",
    "                                          num_rows=max(10,len(data))\n",
    "                                         )],\n",
    "                                 [sg.Button(\"Show percentage of TB_Analysis\"),\n",
    "                                  sg.Button(\"Show percentage of NB_Analysis\")]], grab_anywhere=False)\n",
    "            \n",
    "            while True:\n",
    "                event4, value4 = window4.read()\n",
    "                if event4 == \"Show percentage of TB_Analysis\":\n",
    "                    postweets = df[df.TB_Analysis == 'Positive']\n",
    "                    postweets = postweets['Tweet']\n",
    "                    posp = round(postweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    neutweets = df[df.TB_Analysis == 'Neutral']\n",
    "                    neutweets = neutweets['Tweet']\n",
    "                    neup = round(neutweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    negtweets = df[df.TB_Analysis == 'Negative']\n",
    "                    negtweets = negtweets['Tweet']\n",
    "                    negp = round(negtweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    uiTB()\n",
    "\n",
    "                elif event4 == \"Show percentage of NB_Analysis\":\n",
    "                    postweets = df[df.NB_Analysis == 'Positive']\n",
    "                    postweets = postweets['Tweet']\n",
    "                    posp = round(postweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    neutweets = df[df.NB_Analysis == 'Neutral']\n",
    "                    neutweets = neutweets['Tweet']\n",
    "                    neup = round(neutweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    negtweets = df[df.NB_Analysis == 'Negative']\n",
    "                    negtweets = negtweets['Tweet']\n",
    "                    negp = round(negtweets.shape[0] / df.shape[0] * 100, 1)\n",
    "\n",
    "                    uiNB()\n",
    "                elif event4 == sg.WIN_CLOSED:\n",
    "                    break\n",
    "        elif event == \"Cancel\":\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED:\n",
    "            break\n",
    "    window.close()\n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    \n",
    "\n",
    "\n",
    "def getBarChartTB():\n",
    "    # show the value counts in bar chart\n",
    "    plt.title(\"TB_Analysis Value Counts\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    df['TB_Analysis'].value_counts().plot(kind='bar')\n",
    "    plt.show()\n",
    "def getBarChartNB():\n",
    "    # show the value counts in bar chart\n",
    "    plt.title(\"NB_Analysis Value Counts\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    df['NB_Analysis'].value_counts().plot(kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "def getPieChart():\n",
    "    #Creating PieCart\n",
    "    labels = ['Positive ['+str(posp)+'%]' , 'Neutral ['+str(neup)+'%]','Negative ['+str(negp)+'%]']\n",
    "    sizes = [posp, neup, negp]\n",
    "    colors = [\"yellowgreen\", \"blue\",\"red\"]\n",
    "    patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "    plt.legend(labels)\n",
    "    plt.title(\"Sentiment Analysis Result for keyword = \"+keyword+\"\" )\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    # Define the window's contents\n",
    "    layout = [  [sg.Text(\"Thanks for using this Sentiment Analysis System.\")],\n",
    "              [sg.Text(\"Please wait about 30 seconds for the machine to prepare\")],\n",
    "              [sg.Button('OK')] ]\n",
    "\n",
    "    # Create the window\n",
    "    window = sg.Window('Sentiment Analysis System', layout)\n",
    "    # Display and interact with the Window\n",
    "    event, values = window.read()\n",
    "    # Finish up by removing from the screen\n",
    "    window.close()\n",
    "    #---------------------------------------------------------------------------UI PART\n",
    "    preRUN()\n",
    "    searchInTwitter()\n",
    "    TextBlobSentimentAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pPlot\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as npy\n",
    "from PIL import Image\n",
    "dataset = ['Hello','world','NIHAO','wo','zai','na','ya','ta']\n",
    "def create_word_cloud(string):\n",
    "    maskArray = npy.array(Image.open(\"cloud.png\"))\n",
    "    cloud = WordCloud(background_color = \"white\", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))\n",
    "    cloud.generate(string)\n",
    "    cloud.to_file(\"wordCloud.png\")\n",
    "dataset = str(dataset)\n",
    "create_word_cloud(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a14d1efacd9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtransformed_wine_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwine_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-a14d1efacd9c>\u001b[0m in \u001b[0;36mtransform_format\u001b[1;34m(val)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mwine_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cloud.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2818\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import re, string, random\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import PySimpleGUI as sg\n",
    "from PIL import Image\n",
    "def transform_format(val):\n",
    "    wine_mask = np.array(Image.open(\"cloud.png\"))\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "dataset = ['Hello','world','NIHAO','wo','zai','na','ya','ta']  \n",
    "#def createWordCloud(dataset):\n",
    "# Transform your mask into a new one that will work with the function:\n",
    "transformed_wine_mask = np.ndarray((wine_mask.shape[0],wine_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(wine_mask)):\n",
    "    transformed_wine_mask[i] = list(map(transform_format, wine_mask[i]))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Create a word cloud image\n",
    "wc = WordCloud(background_color=\"white\", max_words=1000, mask=transformed_wine_mask,\n",
    "               stopwords=stopwords, contour_width=3, contour_color='blue')\n",
    "\n",
    "wc.generate(str(dataset))\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"output.png\")\n",
    "# show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4d8025030e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Hello'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'world'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NIHAO'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'zai'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'na'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ya'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcreateWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-7998056e2580>\u001b[0m in \u001b[0;36mcreateWordCloud\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtransformed_wine_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwine_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-7998056e2580>\u001b[0m in \u001b[0;36mtransform_format\u001b[1;34m(val)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mwine_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cloud.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2818\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LazyCorpusLoader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c40138333388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Generate a wordcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# store to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_word_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'LazyCorpusLoader' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = ['Hello','world','NIHAO','wo','zai','na','ya','ta']  \n",
    "wine_mask = np.array(Image.open(\"cloud.png\"))\n",
    "\n",
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "transformed_wine_mask = np.ndarray((wine_mask.shape[0],wine_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(wine_mask)):\n",
    "    transformed_wine_mask[i] = list(map(transform_format, wine_mask[i]))\n",
    "\n",
    "# Create a word cloud image\n",
    "wc = WordCloud(background_color=\"white\", max_words=1000, mask=transformed_wine_mask,\n",
    "               stopwords=stopwords, contour_width=3, contour_color='firebrick')\n",
    "\n",
    "# Generate a wordcloud\n",
    "wc.generate(str(dataset))\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"wordCloud.png\")\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_mask\n",
    "transformed_wine_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
